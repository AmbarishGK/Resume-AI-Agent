{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Resume-AI-Agent This repo is a student-style project that combines: job scraping + job description fetching a local SQLite DB for JDs/resumes a transparent baseline matcher (score + gaps) an interactive Streamlit RAG \"copilot\" for analysis and rewrites Start here: Project Report (high-level summary + what to demo) Getting Started (clone + install) Streamlit Copilot (RAG) (recommended interactive demo)","title":"Home"},{"location":"#resume-ai-agent","text":"This repo is a student-style project that combines: job scraping + job description fetching a local SQLite DB for JDs/resumes a transparent baseline matcher (score + gaps) an interactive Streamlit RAG \"copilot\" for analysis and rewrites Start here: Project Report (high-level summary + what to demo) Getting Started (clone + install) Streamlit Copilot (RAG) (recommended interactive demo)","title":"Resume-AI-Agent"},{"location":"cli/","text":"CLI (ingest + match) The CLI lives in marnow/cli.py . Initialize DB # creates ./marnow.db by default python -m marnow.cli initdb # optional: seed skills (recommended before matching) python -m marnow.cli seed-skills app/data/skills/skills.csv Ingest job descriptions python -m marnow.cli ingest-jd app/data/jds/company-role.txt Ingest resumes python -m marnow.cli ingest-resume app/data/resumes/resume.pdf Find IDs (recommended) If you are matching from the DB, you\u2019ll need resume_id and job_id . python tools/db_cli.py list-resumes --limit 10 python tools/db_cli.py list-jobs --limit 10 # optional filter python tools/db_cli.py list-jobs --contains \"stripe\" --limit 20 Match + report python -m marnow.cli match <resume_id> <job_id> python -m marnow.cli report <match_id> Notes If you need a specific DB path, set: export MARNOW_DB=./marnow.db","title":"CLI (DB/Ingest/Match)"},{"location":"cli/#cli-ingest-match","text":"The CLI lives in marnow/cli.py .","title":"CLI (ingest + match)"},{"location":"cli/#initialize-db","text":"# creates ./marnow.db by default python -m marnow.cli initdb # optional: seed skills (recommended before matching) python -m marnow.cli seed-skills app/data/skills/skills.csv","title":"Initialize DB"},{"location":"cli/#ingest-job-descriptions","text":"python -m marnow.cli ingest-jd app/data/jds/company-role.txt","title":"Ingest job descriptions"},{"location":"cli/#ingest-resumes","text":"python -m marnow.cli ingest-resume app/data/resumes/resume.pdf","title":"Ingest resumes"},{"location":"cli/#find-ids-recommended","text":"If you are matching from the DB, you\u2019ll need resume_id and job_id . python tools/db_cli.py list-resumes --limit 10 python tools/db_cli.py list-jobs --limit 10 # optional filter python tools/db_cli.py list-jobs --contains \"stripe\" --limit 20","title":"Find IDs (recommended)"},{"location":"cli/#match-report","text":"python -m marnow.cli match <resume_id> <job_id> python -m marnow.cli report <match_id>","title":"Match + report"},{"location":"cli/#notes","text":"If you need a specific DB path, set: export MARNOW_DB=./marnow.db","title":"Notes"},{"location":"getting-started/","text":"Getting Started 1) Clone git clone <YOUR_REPO_URL> cd Resume-AI-Agent 2) Create a virtual environment + install dependencies Assumptions: Linux, Python 3.10+. This repo uses uv (recommended), but regular venv works too. Option A: uv (recommended) uv venv source .venv/bin/activate uv pip install -r requirements.txt python -m playwright install chromium Option B: standard venv python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt python -m playwright install chromium 3) Create the database The DB is SQLite. By default it\u2019s created at ./marnow.db . python -m marnow.cli initdb # optional: seed the skills table (used by the matcher) python -m marnow.cli seed-skills app/data/skills/skills.csv If you want the DB somewhere else: export MARNOW_DB=/path/to/marnow.db python -m marnow.cli initdb 4) Run the pipeline python tools/workflow.py Outputs: scraped listings: app/data/jobs/jobs.csv fetched JDs: app/data/jds/*.txt SQLite DB: marnow.db (or whatever MARNOW_DB points to) 5) Ingest a resume + run a match python -m marnow.cli ingest-resume app/data/resumes/resume.pdf # pick a job_id from the DB, then: python -m marnow.cli match <resume_id> <job_id> python -m marnow.cli report <match_id> Optional: run the Streamlit copilot See streamlit-copilot.md for the full setup (requires Ollama + requirements_rag.txt ). Optional: build the docs site mkdocs serve","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#1-clone","text":"git clone <YOUR_REPO_URL> cd Resume-AI-Agent","title":"1) Clone"},{"location":"getting-started/#2-create-a-virtual-environment-install-dependencies","text":"Assumptions: Linux, Python 3.10+. This repo uses uv (recommended), but regular venv works too.","title":"2) Create a virtual environment + install dependencies"},{"location":"getting-started/#option-a-uv-recommended","text":"uv venv source .venv/bin/activate uv pip install -r requirements.txt python -m playwright install chromium","title":"Option A: uv (recommended)"},{"location":"getting-started/#option-b-standard-venv","text":"python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt python -m playwright install chromium","title":"Option B: standard venv"},{"location":"getting-started/#3-create-the-database","text":"The DB is SQLite. By default it\u2019s created at ./marnow.db . python -m marnow.cli initdb # optional: seed the skills table (used by the matcher) python -m marnow.cli seed-skills app/data/skills/skills.csv If you want the DB somewhere else: export MARNOW_DB=/path/to/marnow.db python -m marnow.cli initdb","title":"3) Create the database"},{"location":"getting-started/#4-run-the-pipeline","text":"python tools/workflow.py Outputs: scraped listings: app/data/jobs/jobs.csv fetched JDs: app/data/jds/*.txt SQLite DB: marnow.db (or whatever MARNOW_DB points to)","title":"4) Run the pipeline"},{"location":"getting-started/#5-ingest-a-resume-run-a-match","text":"python -m marnow.cli ingest-resume app/data/resumes/resume.pdf # pick a job_id from the DB, then: python -m marnow.cli match <resume_id> <job_id> python -m marnow.cli report <match_id>","title":"5) Ingest a resume + run a match"},{"location":"getting-started/#optional-run-the-streamlit-copilot","text":"See streamlit-copilot.md for the full setup (requires Ollama + requirements_rag.txt ).","title":"Optional: run the Streamlit copilot"},{"location":"getting-started/#optional-build-the-docs-site","text":"mkdocs serve","title":"Optional: build the docs site"},{"location":"project-report/","text":"Mini project report Problem When tailoring a resume to a job description (JD), it\u2019s easy to miss keywords, responsibilities, and domain cues that an ATS or recruiter expects. Doing this manually is slow and inconsistent. Goal Build a reproducible pipeline that: collects job postings + full job descriptions stores JDs and resumes in a local database produces a transparent match score + gap list provides an interactive Streamlit \"copilot\" UI that can analyze/rewrite a resume for a specific JD System overview The repo has three layers: 1) Data collection Scraper : tools/jobscraper/main.py (Playwright) JD fetcher : tools/make_jds_from_jobs.py (requests + BeautifulSoup) Orchestrator : tools/workflow.py runs scrape -> fetch -> ingest 2) Storage + ingestion SQLite DB: marnow.db (or MARNOW_DB ) Ingestion utilities: marnow/ingest.py CLI: marnow/cli.py 3) Matching + copilot Heuristic matcher : marnow/match.py computes component scores (skills overlap, responsibilities overlap, seniority/domain heuristics) outputs missing skills (top gaps) RAG + Copilot server : tools/rag_resume_server.py (FastAPI + Chroma + Ollama) Streamlit UI : tools/rag_resume_app.py upload resume PDF + paste JD text check score, apply suggested changes, and export LaTeX/PDF How to run (recommended demo path) A) Pipeline demo (scrape -> fetch -> ingest) python tools/workflow.py Expected artifacts: - app/data/jobs/jobs.csv - app/data/jds/*.txt - marnow.db B) Matching demo (resume -> match -> report) python -m marnow.cli initdb python -m marnow.cli seed-skills app/data/skills/skills.csv python -m marnow.cli ingest-resume /path/to/your_resume.pdf python tools/db_cli.py list-resumes --limit 10 python tools/db_cli.py list-jobs --limit 10 python -m marnow.cli match <resume_id> <job_id> python -m marnow.cli report <match_id> C) Streamlit copilot demo (interactive) See Streamlit Copilot (RAG) for the full steps. Limitations / notes Scraping is config-driven but career sites change often; selectors may need updates. The baseline match score is heuristic (intended to be transparent and fast). The RAG/copilot features depend on a local Ollama setup and model availability. Artifacts Code: tools/ , marnow/ , app/config/ Data artifacts: app/data/jobs/jobs.csv , app/data/jds/*.txt DB artifact (generated): marnow.db","title":"Project Report"},{"location":"project-report/#mini-project-report","text":"","title":"Mini project report"},{"location":"project-report/#problem","text":"When tailoring a resume to a job description (JD), it\u2019s easy to miss keywords, responsibilities, and domain cues that an ATS or recruiter expects. Doing this manually is slow and inconsistent.","title":"Problem"},{"location":"project-report/#goal","text":"Build a reproducible pipeline that: collects job postings + full job descriptions stores JDs and resumes in a local database produces a transparent match score + gap list provides an interactive Streamlit \"copilot\" UI that can analyze/rewrite a resume for a specific JD","title":"Goal"},{"location":"project-report/#system-overview","text":"The repo has three layers:","title":"System overview"},{"location":"project-report/#1-data-collection","text":"Scraper : tools/jobscraper/main.py (Playwright) JD fetcher : tools/make_jds_from_jobs.py (requests + BeautifulSoup) Orchestrator : tools/workflow.py runs scrape -> fetch -> ingest","title":"1) Data collection"},{"location":"project-report/#2-storage-ingestion","text":"SQLite DB: marnow.db (or MARNOW_DB ) Ingestion utilities: marnow/ingest.py CLI: marnow/cli.py","title":"2) Storage + ingestion"},{"location":"project-report/#3-matching-copilot","text":"Heuristic matcher : marnow/match.py computes component scores (skills overlap, responsibilities overlap, seniority/domain heuristics) outputs missing skills (top gaps) RAG + Copilot server : tools/rag_resume_server.py (FastAPI + Chroma + Ollama) Streamlit UI : tools/rag_resume_app.py upload resume PDF + paste JD text check score, apply suggested changes, and export LaTeX/PDF","title":"3) Matching + copilot"},{"location":"project-report/#how-to-run-recommended-demo-path","text":"","title":"How to run (recommended demo path)"},{"location":"project-report/#a-pipeline-demo-scrape-fetch-ingest","text":"python tools/workflow.py Expected artifacts: - app/data/jobs/jobs.csv - app/data/jds/*.txt - marnow.db","title":"A) Pipeline demo (scrape -&gt; fetch -&gt; ingest)"},{"location":"project-report/#b-matching-demo-resume-match-report","text":"python -m marnow.cli initdb python -m marnow.cli seed-skills app/data/skills/skills.csv python -m marnow.cli ingest-resume /path/to/your_resume.pdf python tools/db_cli.py list-resumes --limit 10 python tools/db_cli.py list-jobs --limit 10 python -m marnow.cli match <resume_id> <job_id> python -m marnow.cli report <match_id>","title":"B) Matching demo (resume -&gt; match -&gt; report)"},{"location":"project-report/#c-streamlit-copilot-demo-interactive","text":"See Streamlit Copilot (RAG) for the full steps.","title":"C) Streamlit copilot demo (interactive)"},{"location":"project-report/#limitations-notes","text":"Scraping is config-driven but career sites change often; selectors may need updates. The baseline match score is heuristic (intended to be transparent and fast). The RAG/copilot features depend on a local Ollama setup and model availability.","title":"Limitations / notes"},{"location":"project-report/#artifacts","text":"Code: tools/ , marnow/ , app/config/ Data artifacts: app/data/jobs/jobs.csv , app/data/jds/*.txt DB artifact (generated): marnow.db","title":"Artifacts"},{"location":"setup/","text":"Setup notes This page mirrors SETUP.md (kept in the repo root). Quick start (uv) uv venv source .venv/bin/activate uv pip install -r requirements.txt python -m playwright install chromium Alternative (venv) python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt python -m playwright install chromium","title":"Setup Notes"},{"location":"setup/#setup-notes","text":"This page mirrors SETUP.md (kept in the repo root).","title":"Setup notes"},{"location":"setup/#quick-start-uv","text":"uv venv source .venv/bin/activate uv pip install -r requirements.txt python -m playwright install chromium","title":"Quick start (uv)"},{"location":"setup/#alternative-venv","text":"python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt python -m playwright install chromium","title":"Alternative (venv)"},{"location":"streamlit-copilot/","text":"Streamlit Copilot (RAG) This is the most interactive way to use the project. It runs a local FastAPI server + Streamlit UI. What it does Upload a resume PDF and paste a job description (JD) Index both into a local vector store (Chroma) Ask questions (RAG) and run copilot-style actions: score rewrite (apply suggested changes and rescore) export LaTeX / PDF Prerequisites 1) Install optional dependencies uv pip install -r requirements_rag.txt 2) Start Ollama + pull models This project uses Ollama for embeddings and generation. ollama serve # recommended defaults used by the server/UI ollama pull nomic-embed-text ollama pull llama3 ollama pull mistral:instruct ollama pull llama2:13b You can change models with env vars: OLLAMA_EMBED_MODEL (default: nomic-embed-text ) OLLAMA_LLM_MODEL (default: llama3 ) Run 1) Start the backend uv run uvicorn tools.rag_resume_server:app --reload --port 8100 2) Start the Streamlit UI uv run streamlit run tools/rag_resume_app.py --server.port 8502 Open the UI at http://127.0.0.1:8502 . Typical flow in the UI 1) Ingest - Upload resume PDF - Paste JD text - Click Ingest resume + JD The backend will create/dedupe DB rows in marnow.db and return resume_id and job_id automatically. 2) Check score - Click Check score to see the baseline heuristic score and missing skills. 3) Apply suggested changes - Click Apply suggested changes to generate a rewritten version and rescore. - A new resume_id is created for the rewritten resume. 4) Download - Download .tex or .pdf (PDF requires pdflatex ). Notes Vector store directory is controlled by RAG_RESUME_CHROMA_DIR (default ./rag_resume_chroma ). SQLite DB path is controlled by MARNOW_DB (default ./marnow.db ).","title":"Streamlit Copilot (RAG)"},{"location":"streamlit-copilot/#streamlit-copilot-rag","text":"This is the most interactive way to use the project. It runs a local FastAPI server + Streamlit UI.","title":"Streamlit Copilot (RAG)"},{"location":"streamlit-copilot/#what-it-does","text":"Upload a resume PDF and paste a job description (JD) Index both into a local vector store (Chroma) Ask questions (RAG) and run copilot-style actions: score rewrite (apply suggested changes and rescore) export LaTeX / PDF","title":"What it does"},{"location":"streamlit-copilot/#prerequisites","text":"","title":"Prerequisites"},{"location":"streamlit-copilot/#1-install-optional-dependencies","text":"uv pip install -r requirements_rag.txt","title":"1) Install optional dependencies"},{"location":"streamlit-copilot/#2-start-ollama-pull-models","text":"This project uses Ollama for embeddings and generation. ollama serve # recommended defaults used by the server/UI ollama pull nomic-embed-text ollama pull llama3 ollama pull mistral:instruct ollama pull llama2:13b You can change models with env vars: OLLAMA_EMBED_MODEL (default: nomic-embed-text ) OLLAMA_LLM_MODEL (default: llama3 )","title":"2) Start Ollama + pull models"},{"location":"streamlit-copilot/#run","text":"","title":"Run"},{"location":"streamlit-copilot/#1-start-the-backend","text":"uv run uvicorn tools.rag_resume_server:app --reload --port 8100","title":"1) Start the backend"},{"location":"streamlit-copilot/#2-start-the-streamlit-ui","text":"uv run streamlit run tools/rag_resume_app.py --server.port 8502 Open the UI at http://127.0.0.1:8502 .","title":"2) Start the Streamlit UI"},{"location":"streamlit-copilot/#typical-flow-in-the-ui","text":"1) Ingest - Upload resume PDF - Paste JD text - Click Ingest resume + JD The backend will create/dedupe DB rows in marnow.db and return resume_id and job_id automatically. 2) Check score - Click Check score to see the baseline heuristic score and missing skills. 3) Apply suggested changes - Click Apply suggested changes to generate a rewritten version and rescore. - A new resume_id is created for the rewritten resume. 4) Download - Download .tex or .pdf (PDF requires pdflatex ).","title":"Typical flow in the UI"},{"location":"streamlit-copilot/#notes","text":"Vector store directory is controlled by RAG_RESUME_CHROMA_DIR (default ./rag_resume_chroma ). SQLite DB path is controlled by MARNOW_DB (default ./marnow.db ).","title":"Notes"},{"location":"troubleshooting/","text":"Troubleshooting Playwright browser not found python -m playwright install chromium Database path issues export MARNOW_DB=./marnow.db Workflow debugging Try running each step by itself: python tools/jobscraper/main.py --config app/config/careers.yaml --out app/data/jobs/jobs.csv python tools/make_jds_from_jobs.py --limit 2 python -m marnow.cli initdb","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#playwright-browser-not-found","text":"python -m playwright install chromium","title":"Playwright browser not found"},{"location":"troubleshooting/#database-path-issues","text":"export MARNOW_DB=./marnow.db","title":"Database path issues"},{"location":"troubleshooting/#workflow-debugging","text":"Try running each step by itself: python tools/jobscraper/main.py --config app/config/careers.yaml --out app/data/jobs/jobs.csv python tools/make_jds_from_jobs.py --limit 2 python -m marnow.cli initdb","title":"Workflow debugging"},{"location":"workflow/","text":"Pipeline (workflow) The entry point is tools/workflow.py . It orchestrates: scrape jobs -> app/data/jobs/jobs.csv fetch job descriptions -> app/data/jds/*.txt ingest JDs into SQLite -> marnow.db (or MARNOW_DB ) Note: workflow.py will create the DB if needed, but for manual usage you can run python -m marnow.cli initdb . Basic run python tools/workflow.py Useful flags # fetch all JDs and ingest all python tools/workflow.py --all-jds --all-ingest # filter during scraping python tools/workflow.py --include \"software,engineer\" --exclude \"senior,staff\" --locations \"remote,sf\" # reuse existing artifacts python tools/workflow.py --skip-scrape --skip-fetch Related docs See setup.md for dependency install See troubleshooting.md if scraping/fetching fails","title":"Pipeline (Workflow)"},{"location":"workflow/#pipeline-workflow","text":"The entry point is tools/workflow.py . It orchestrates: scrape jobs -> app/data/jobs/jobs.csv fetch job descriptions -> app/data/jds/*.txt ingest JDs into SQLite -> marnow.db (or MARNOW_DB ) Note: workflow.py will create the DB if needed, but for manual usage you can run python -m marnow.cli initdb .","title":"Pipeline (workflow)"},{"location":"workflow/#basic-run","text":"python tools/workflow.py","title":"Basic run"},{"location":"workflow/#useful-flags","text":"# fetch all JDs and ingest all python tools/workflow.py --all-jds --all-ingest # filter during scraping python tools/workflow.py --include \"software,engineer\" --exclude \"senior,staff\" --locations \"remote,sf\" # reuse existing artifacts python tools/workflow.py --skip-scrape --skip-fetch","title":"Useful flags"},{"location":"workflow/#related-docs","text":"See setup.md for dependency install See troubleshooting.md if scraping/fetching fails","title":"Related docs"}]}